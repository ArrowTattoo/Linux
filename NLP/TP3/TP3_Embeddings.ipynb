{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4VhjAzcrk7g"
      },
      "source": [
        "#Compte-Rendu du TP3 : Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh2LQNW7rR3r"
      },
      "source": [
        "##1. Introduction\n",
        "\n",
        "Ce TP porte sur la mise en place d’un modèle de représentation des mots plus avancé que\n",
        "l’approche TF-IDF : nous utiliserons ici l’approche par réseaux de neurones word2vec. Nous\n",
        "chercherons dans un premier temps à comparer les vecteur et ensuite à créer notre propre modèle de\n",
        "représentation des mots par word embeddings au moyen de la librairie Gensim.\n",
        "\n",
        "Vous aurez besoin\n",
        "de la librairie word2vec (https://radimrehurek.com/gensim/models/word2vec.html) et également de\n",
        "simple_process pour réaliser le retraitement de chaque ligne (https://tedboy.github.io/nlps/generated/generated/gensim.utils.simple_preprocess.html).\n",
        "\n",
        "\n",
        "Nous chercherons ensuite à visualiser les embeddings sur deux dimensions (réduction de dimension\n",
        "pour pouvoir les « interpréter »). Nous utiliserons l’algorithme t-SNE (https://fr.wikipedia.org/wiki/Algorithme_t-SNE). Il faudra alors utiliser la librairie TSNE de sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). Il faudra également utiliser la librairie pat\n",
        "de pyplot pour visualiser les graphiques (https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mxUq2oFlrGIu"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq2Go_LNsBDu"
      },
      "source": [
        "## 2. Recherche des mots proches et opérations arithmétiques\n",
        "**Modèle.** Afin de nous intéresser aux embeddings et à leur comparaison, nous allons dans un premier temps récupérer un modèle pré-entrainé en français. Nous allons par exemple récupérer ce\n",
        "modèle : https://huggingface.co/Word2vec/wikipedia2vec_frwiki_20180420_300d\n",
        "N’hésitez pas à aller voir la librairie KeyedVectors de Gensim : https://radimrehurek.com/gensim_3.8.3/models/keyedvectors.html\n",
        "\n",
        "1. Vous avez normalement des informations sur comment l’utiliser. Récupérez et chargez ensuite\n",
        "ce modèle. Réalisez ensuite plusieurs tests avec différents mots pour voir les mots similaires.\n",
        "Essayez notamment d’utiliser des majuscules/minuscules, des noms courants, des noms plus\n",
        "rares…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "292767c5786c42e2832b9fecb9f05bcf",
            "2fa00ed3343a415fb6921b92e1f51478",
            "583c8fdfe640476e82d4de0aca905d06",
            "22de82f480144ad685968a3c15463256",
            "8b5e1a191d794e24b2c32af929592dfa",
            "8db2ae1397554bafa4df39e311a4b8da",
            "9665473bb7d245309e79a3b5d9a31014",
            "33e00191e4b649c7b129327431a5a2da",
            "f8f84735b8bb4cc7bc9b7b8ef667a29e",
            "e54afc365e8e4184868a83b9624ddb70",
            "381d0506ac1348aa93a561e2bd4a9408"
          ]
        },
        "id": "cpUbCnjNsAyv",
        "outputId": "ad1c0a7c-3007-4f30-b876-664c9485364e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ait', 0.6869087219238281),\n",
              " ('ayant', 0.6397339701652527),\n",
              " ('aient', 0.6187543869018555),\n",
              " ('être', 0.6099869608879089),\n",
              " ('avait', 0.6030339598655701),\n",
              " ('a', 0.5999142527580261),\n",
              " ('aura', 0.5903619527816772),\n",
              " ('après', 0.5864975452423096),\n",
              " ('aurait', 0.5824207067489624),\n",
              " ('eût', 0.5650670528411865)]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from huggingface_hub import hf_hub_download\n",
        "model = KeyedVectors.load_word2vec_format(hf_hub_download(repo_id=\"Word2vec/wikipedia2vec_frwiki_20180420_300d\", filename=\"frwiki_20180420_300d.txt\"),limit=100000)\n",
        "\n",
        "model.most_similar(\"avoir\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCBfXdtrsLfu"
      },
      "source": [
        "2. Vous pouvez ensuite découvrir la puissance des embeddings au travers des opérateurs\n",
        "arithmétiques. Essayez par exemple : model.most_similar(model.get_vector(\"roi\")-\n",
        "model.get_vector(\"homme\")+model.get_vector(\"femme\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('reine', 0.6593815088272095),\n",
              " ('régente', 0.5754857063293457),\n",
              " ('princesse', 0.5725846886634827),\n",
              " ('rois', 0.5570774078369141),\n",
              " ('maîtresse', 0.5461241602897644),\n",
              " ('fiancée', 0.5319637060165405),\n",
              " ('douairière', 0.5309650897979736),\n",
              " ('répudiée', 0.5255407094955444),\n",
              " ('fille', 0.5240213871002197),\n",
              " ('épouse', 0.5179541110992432)]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec_roi = model['roi']      \n",
        "vec_homme = model['homme']  \n",
        "vec_femme = model['femme']\n",
        "vec_roi - vec_homme + vec_femme\n",
        "model.most_similar(positive=['roi', 'femme'], negative=['homme'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1WW8P_psgFX"
      },
      "source": [
        "##3. Création d’un modèle word2vec\n",
        "\n",
        "**Données.** Nous allons maintenant créer notre propre modèle word2vec. Pour cela, nous avons\n",
        "besoin de données textuelles. Voici un exemple de jeu de données que vous pouvez utiliser : http://redac.univ-tlse2.fr/corpus/wikipedia.html\n",
        "Vous pouvez notamment ici utiliser les librairies Word2Vec et simple_preprocess de Gensim (voir introduction). Vous allez devoir :\n",
        "1. Ouvrir le fichier de données textuelles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "/bin/bash: line 1: 7z: command not found\n",
            "iconv: cannot open input file `wikipediaTXT.txt': No such file or directory\n",
            "head: cannot open 'wikipediaFR-TXT.utf8.txt' for reading: No such file or directory\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "you must first build vocabulary before training the model",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m processed_text = [simple_preprocess(sentence, min_len=\u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#train model w2v\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m model = \u001b[43mWord2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessed_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_count\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/gensim/models/word2vec.py:430\u001b[39m, in \u001b[36mWord2Vec.__init__\u001b[39m\u001b[34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[39m\n\u001b[32m    428\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_corpus_sanity(corpus_iterable=corpus_iterable, corpus_file=corpus_file, passes=(epochs + \u001b[32m1\u001b[39m))\n\u001b[32m    429\u001b[39m     \u001b[38;5;28mself\u001b[39m.build_vocab(corpus_iterable=corpus_iterable, corpus_file=corpus_file, trim_rule=trim_rule)\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcorpus_total_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mend_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmin_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trim_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/gensim/models/word2vec.py:1045\u001b[39m, in \u001b[36mWord2Vec.train\u001b[39m\u001b[34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28mself\u001b[39m.min_alpha = end_alpha \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.min_alpha\n\u001b[32m   1043\u001b[39m \u001b[38;5;28mself\u001b[39m.epochs = epochs\n\u001b[32m-> \u001b[39m\u001b[32m1045\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_training_sanity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[38;5;28mself\u001b[39m._check_corpus_sanity(corpus_iterable=corpus_iterable, corpus_file=corpus_file, passes=epochs)\n\u001b[32m   1048\u001b[39m \u001b[38;5;28mself\u001b[39m.add_lifecycle_event(\n\u001b[32m   1049\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1050\u001b[39m     msg=(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1054\u001b[39m     ),\n\u001b[32m   1055\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/gensim/models/word2vec.py:1554\u001b[39m, in \u001b[36mWord2Vec._check_training_sanity\u001b[39m\u001b[34m(self, epochs, total_examples, total_words, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     logger.warning(\u001b[33m\"\u001b[39m\u001b[33mEffective \u001b[39m\u001b[33m'\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m'\u001b[39m\u001b[33m higher than previous training cycles\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.wv.key_to_index:  \u001b[38;5;66;03m# should be set by `build_vocab`\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1554\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33myou must first build vocabulary before training the model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1555\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.wv.vectors):\n\u001b[32m   1556\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33myou must initialize vectors before training the model\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mRuntimeError\u001b[39m: you must first build vocabulary before training the model"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "import wget\n",
        "wget.download('http://redac.univ-tlse2.fr/corpus/wikipedia/wikipediaFR-TXT.txt.7z','wikipediaFR-TXT.txt.7z')\n",
        "\n",
        "#Pour dezipper\n",
        "!7z x wikipediaFR-TXT.txt.7z\n",
        "\n",
        "#Pour convertir en utf8\n",
        "!iconv -f ISO-8859-1 -t utf-8 wikipediaTXT.txt -o wikipediaFR-TXT.utf8.txt\n",
        "\n",
        "#Pour prendre les 10k premières lignes\n",
        "!head -n 10000 wikipediaFR-TXT.utf8.txt > wikipediaFR-TXT.10k.utf8.txt\n",
        "\n",
        "#Pour simple process\n",
        "file_path = \"wikipediaFR-TXT.10k.utf8.txt\"\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "   text = file.readlines()\n",
        "\n",
        "processed_text = [simple_preprocess(sentence, min_len=1) for sentence in text]\n",
        "\n",
        "#train model w2v\n",
        "model = Word2Vec(sentences=processed_text, vector_size=100, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYqBrePXsoLm"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'wikipediaFR-TXT.10k.utf8.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m filename = \u001b[33m\"\u001b[39m\u001b[33mwikipediaFR-TXT.10k.utf8.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m     text = f.read()\n\u001b[32m      4\u001b[39m sentences = text.split(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'wikipediaFR-TXT.10k.utf8.txt'"
          ]
        }
      ],
      "source": [
        "filename = \"wikipediaFR-TXT.10k.utf8.txt\"\n",
        "with open(filename, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "sentences = text.split('.')\n",
        "sentences = [simple_preprocess(sentence) for sentence in sentences]\n",
        "model = Word2Vec(sentences, vector_size=300, window=5, min_count=5, workers=4)\n",
        "model.wv.most_similar(\"roi\")  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFfKVrOostVV"
      },
      "source": [
        "2. Prétraiter les données avec simple_preprocess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzqQKPlDsoiF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDmlS-WnsvLu"
      },
      "source": [
        "3. Entrainer le modèle word2vec avec la fonction Word2Vec()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqzErpm1spHl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IfFlpHCswxO"
      },
      "source": [
        "4. Sauvegarder le modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEWTJjECso3o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dpmCWrCszNN"
      },
      "source": [
        "5. Regarder que tout a bien fonctionné en réutilisant la visualisation des vecteurs comme dans la partie 2 Recherche des mots proches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_ne4mmqs0IU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MattS6gEtPSt"
      },
      "source": [
        "## 4. Visualisation des vecteurs avec tSNE et matplotlib\n",
        "**Modèles.** Vous pouvez utiliser ici le premier modèle pré-entrainé récupéré ou celui que vous avez\n",
        "entrainé de votre côté.\n",
        "Concrètement, deux étapes vont être réalisées : une réduction de dimension (ici, on obtient deux\n",
        "dimensions pour une visualisation des points en 2D) puis un affichage de ces points.\n",
        "1. Afin de pouvoir avoir quelque chose de visualisable, nous n’allons pas mettre tous les mots\n",
        "disponibles sur le graphique (à vous de choisir le nombre). Vous pouvez utiliser le code suivant\n",
        "ou refaire de votre côté une liste de mots à afficher. Ici, nous cherchons les n mots les plus\n",
        "fréquents :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYPxqp5ntW2s"
      },
      "outputs": [],
      "source": [
        "mots_outils = [\"le\", \"la\", \"les\", \"de\", \"du\", \"des\", \"en\", \"et\", \"à\", \"au\", \"aux\", \"un\", \"une\", \"ce\",\n",
        "\"cet\", \"cette\", \"ces\", \"qui\", \"que\", \"quoi\", \"qu\", \"dont\", \"où\", \"si\", \"s\", \"ne\", \"pas\", \"plus\", \"moins\",\n",
        "\"mais\", \"ou\", \"et\", \"donc\", \"or\", \"ni\", \"car\", \"aussi\", \"après\", \"avant\", \"pendant\", \"depuis\", \"alors\",\n",
        "\"lorsque\", \"lors\", \"après\", \"avant\", \"enfin\", \"puis\", \"ensuite\", \"aujourd'hui\", \"demain\", \"hier\",\n",
        "\"maintenant\", \"parfois\", \"quelquefois\", \"souvent\", \"jamais\", \"toujours\", \"peu\", \"beaucoup\", \"trop\",\n",
        "\"assez\", \"plusieurs\", \"tout\", \"rien\", \"personne\", \"autre\", \"même\", \"chacun\", \"chacune\", \"ensemble\",\n",
        "\"seul\", \"seule\", \"autre\", \"autres\", \"chaque\", \"plusieurs\", \"quelques\", \"tant\", \"si\", \"tellement\", \"tel\",\n",
        "\"telle\", \"telles\", \"tels\", \"toute\", \"tous\", \"tout\", \"rien\", \"personne\", \"quelqu'un\", \"quelqu'une\"]\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from collections import Counter\n",
        "# Chargement du modèle\n",
        "model = Word2Vec.load(\"modele_word2vec.model\")\n",
        "# Fréquence des mots\n",
        "word_freq = Counter(model.wv.index_to_key)\n",
        "top_100_words = [word for word, freq in word_freq.most_common(500) if word not in mots_outils]\n",
        "# Extraire les vecteurs d'embedding des 100 mots les plus fréquents\n",
        "vectors = [model.wv[word] for word in top_100_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_rqcwuhtih9"
      },
      "source": [
        "2. Vous allez maintenant réaliser la réduction de dimension de cette liste. Je vous invite à regarder la librairie TSNE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9oPQDgItpHl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDWLVtMYtmZw"
      },
      "source": [
        "3. Positionnez ensuite les points avec matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UzEADNPtrRg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6iEUbOVttcZ"
      },
      "source": [
        "## 5. Classification avec des représentations word2vec\n",
        "\n",
        "Reprenez ce que vous avez fait dans le TP 2 en prenant des représentations word2vec pour l’entrainement du classifieur."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "22de82f480144ad685968a3c15463256": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e54afc365e8e4184868a83b9624ddb70",
            "placeholder": "​",
            "style": "IPY_MODEL_381d0506ac1348aa93a561e2bd4a9408",
            "value": " 5.68G/5.68G [02:20&lt;00:00, 47.3MB/s]"
          }
        },
        "292767c5786c42e2832b9fecb9f05bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fa00ed3343a415fb6921b92e1f51478",
              "IPY_MODEL_583c8fdfe640476e82d4de0aca905d06",
              "IPY_MODEL_22de82f480144ad685968a3c15463256"
            ],
            "layout": "IPY_MODEL_8b5e1a191d794e24b2c32af929592dfa"
          }
        },
        "2fa00ed3343a415fb6921b92e1f51478": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8db2ae1397554bafa4df39e311a4b8da",
            "placeholder": "​",
            "style": "IPY_MODEL_9665473bb7d245309e79a3b5d9a31014",
            "value": "frwiki_20180420_300d.txt: 100%"
          }
        },
        "33e00191e4b649c7b129327431a5a2da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "381d0506ac1348aa93a561e2bd4a9408": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "583c8fdfe640476e82d4de0aca905d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33e00191e4b649c7b129327431a5a2da",
            "max": 5680324558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8f84735b8bb4cc7bc9b7b8ef667a29e",
            "value": 5680324558
          }
        },
        "8b5e1a191d794e24b2c32af929592dfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8db2ae1397554bafa4df39e311a4b8da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9665473bb7d245309e79a3b5d9a31014": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e54afc365e8e4184868a83b9624ddb70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8f84735b8bb4cc7bc9b7b8ef667a29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
