{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90602f82-b859-456c-ba13-d4287adfaaa4",
   "metadata": {},
   "source": [
    "# TP1: Outils pour le TALN\n",
    "\n",
    "* Cours: Traitement du langage naturel\n",
    "* Auteur: Ygor GALLINA\n",
    "* Date: Janvier 2024\n",
    "\n",
    "## Préambule\n",
    "\n",
    "Le but de ce TP est d’appréhender et de prendre en main les outils de traitement automatique de la langue existant pour traiter des données textuelles.\n",
    "\n",
    "Notre cas d’usage est d'analyser les discours de nouvelle année des présidents de la république française. Ces discours sont disponible sur la plateforme [vie-publique.fr](https://www.vie-publique.fr/qui-sommes-nous) qui recense (entre autre) les discours provenant du gouvernement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1894458-e80a-4303-ae69-60e6cb08b30d",
   "metadata": {},
   "source": [
    "### Google Collab\n",
    "\n",
    "Si vous executez ce notebook avec Google Collab exécutez la commande suivante pour connaître le pays dans lequel se trouve le serveur qui exécute votre code. Naviguez ensuite vers [electricitymaps.com](https://app.electricitymaps.com/map) pour connaitre le mix electrique de ce pays.\n",
    "\n",
    "> Le **mix énergétique**, ou bouquet énergétique, est la **répartition** des différentes **sources d'énergies** primaires **consommées** dans une zone géographique donnée. ... Le **mix électrique**, avec lequel il ne doit pas être confondu, ne prend en compte que **les sources d'énergie contribuant à la production d'électricité** ; or l'électricité ne représente que 18,5 % de la consommation finale d'énergie au niveau mondial.  Source: [Wikipédia](https://fr.wikipedia.org/wiki/Mix_%C3%A9nerg%C3%A9tique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1caf8ef8-ce52-462f-b890-05236f9f1ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ip\": \"20.61.126.208\",\n",
      "  \"city\": \"Amsterdam\",\n",
      "  \"region\": \"North Holland\",\n",
      "  \"country\": \"NL\",\n",
      "  \"loc\": \"52.3740,4.8897\",\n",
      "  \"org\": \"AS8075 Microsoft Corporation\",\n",
      "  \"postal\": \"1012\",\n",
      "  \"timezone\": \"Europe/Amsterdam\",\n",
      "  \"readme\": \"https://ipinfo.io/missingauth\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "!curl ipinfo.io\n",
    "# Ou bien\n",
    "# !curl https://api.country.is/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba5203-7624-437f-9931-3fc3516d4cc5",
   "metadata": {},
   "source": [
    "## Exercice 1: Pré-traitements\n",
    "\n",
    "L'objectif de cet exercice est de se familiariser avec les différents pré-traitement utilisés dans le TALN. Pour cela n'hésitez pas a consulter la documentation de chacune des librairies pour comprendre comment elles fonctionnent et à quoi correspondent les arguments de leurs fonctions.\n",
    "\n",
    "L'objectif de ce TP est de chercher dans un corpus de document les phrases qui traitent de montagnes. Pour cela différentes techniques de traitement automatique des langues (TAL, en: NLP) devront être utilisées: la segmentation en phrase, en tokens, la normalisation, l'étiquetage morphosyntaxique.\n",
    "\n",
    "## Prise en main du corpus\n",
    "\n",
    "Un projet de TAL commence toujours par le choix d'un corpus et son exploration. Nous utiliserons ici les .....\n",
    "\n",
    "### Bash\n",
    "\n",
    "Les outils en ligne de commande permettent des traitement simples et rapide à  effectuer.\n",
    "\n",
    "La commande `sed` permet de remplacer un motif par une chaîne de caractère, elle fonctionne ligne par ligne, elle s'utilise de la façon suivante:\n",
    "`sed 's/MOTIF/REMPLACEMENT/g'`, le `s` signifie substitution et le `g` global ce sont des drapeau (flags). Les caractères `/` séparent les différentes parties de la commande et peuvent être n'importe quel autre caractère (`sed 's°chats?°chat°g'` est une commande valide).\n",
    "\n",
    "La commande `grep` permet de filtrer les lignes d'un fichier suivant un motif.\n",
    "\n",
    "N'hésitez pas à consulter le `man`uel des commandes pour plus d'information (pour rechercher dans une page de manuel: taper `/`, écrire un mot, valider avec entrée, taper `n` pour la prochaine occurence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60446e-88d7-4be9-975f-8e9a3d083c44",
   "metadata": {},
   "source": [
    "1. Quel est le nombre de lignes et de mots dans l'ensemble des documents ? (commande `wc`)\n",
    "    - Votre réponse\n",
    "2. Que fait cette commande ? `cat *.txt| sed -E 's/([[:alnum:]])([\\?\\!.])/\\1 \\2/g' | sed -E 's/ +/\\n/g'`.\n",
    "    - Votre réponse\n",
    "3. A l'aide des commandes `uniq` et `sort` afficher les 10 tokens les plus fréquents.\n",
    "    - Votre réponse\n",
    "4. Combien de types (tokens unique) comporte le texte ?\n",
    "    - Votre réponse\n",
    "5. En regardant les tokens, identifiez en 2 qui pourraient être mieux segmentés.\n",
    "    - Votre réponse\n",
    "6. A l'aide de la commande `grep` selectionnez les types de tokens que vous avez identifié à l'étape précedente. Donnez 3 exemples de chaque.\n",
    "    - Vos réponses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19568390",
   "metadata": {},
   "source": [
    "Q1 On a total 885 lignes et total 28408 mots\n",
    "\n",
    "Q2 On insérer espaces entre dans les mots et ponctuations(. ? !) et apres chaque espaces par un retour à la ligne.Donc,divisez le texte d'une phrase dans un format mot par ligne.\n",
    "\n",
    "Q3 cat *.txt | sed -E 's/([[:alnum:]])([\\?\\!.])/\\1 \\2/g' | tr '[:space:]' '\\n' | tr '[:upper:]' '[:lower:]' | grep -v '^$' | sort | uniq -c | sort -nr | head -10\n",
    "\n",
    "Q4 cat *.txt | sed -E 's/([[:alnum:]])([\\?\\!.])/\\1 \\2/g' | tr '[:space:]' '\\n'  | grep -v '^$' | sort -u | wc -l\n",
    "5440\n",
    "\n",
    "Q5 les mot connect avec \" ' \" comme j'ai ou les mots avec trait.\n",
    "\n",
    "Q6 cat *.txt | sed -E 's/([[:alnum:]])([\\?\\!.])/\\1 \\2/g' | tr '[:space:]' '\\n'  | grep -v '^$' | grep -i \"[[:alnum:]]*'[[:alnum:]]*\"\n",
    "\n",
    "   cat *.txt | sed -E 's/([[:alnum:]])([\\?\\!.])/\\1 \\2/g' | tr '[:space:]' '\\n'  | grep -v '^$' | grep -i \"[[:alnum:]]*-[[:alnum:]]*\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf8adad-98dd-409d-bc80-96790806cba4",
   "metadata": {},
   "source": [
    "### Python et NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465af96e-749c-426a-884f-7f3318838cdd",
   "metadata": {},
   "source": [
    "1. Téléchargez les données si ce n'est pas déjà fait et ouvrez un notebook à l'aide de la commande `jupyter notebook`.\n",
    "2. Chargez les données à l'aide du code ci-dessous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60235157-626c-4bbf-ae5a-abee5e961b18",
   "metadata": {},
   "source": [
    "3. Utilisez la bibliothèque nltk et la fonction `nltk.word_tokenize` pour tokeniser le corpus.\n",
    "   * Est-ce que les tokens qui étaient mal segmentés à la question 6. le sont toujours ?\n",
    "    - Vous pouvez chercher dans une liste avec une compréhension de liste comme `[t for t in MONVOCAB.items() if 'chat' in t]`)\n",
    "    - Ou encore en écrivant tout les mots dans un fichier, que vous pourrez parcourir à l'aide d'un éditeur de texte.\n",
    "- Votre réponse\n",
    "\n",
    "\n",
    "4. Ecrivez ensuite une fonction `pretreat` qui prend en entrée un document tokénisé et renvoie pour chaque mot son étiquette morpho-syntaxique (ou POS tag) ainsi que sa version racinisée (ou stem).\n",
    "   * Un document sera de la forme `[('TOKEN', 'POSTAG', 'STEM'), ('TOKEN', 'POSTAG', 'STEM'), ...]`\n",
    "   * Pour les étiquettes morpho-syntaxiques vous pourrez utiliser la fonction `nltk.pos_tag` (les étiquettes résultant de cette fonction proviennent de l'universal dependencies et sont explicités sur [cette page](https://universaldependencies.org/u/pos/index.html), ce jeu d'étiquette est commun à l'ensemble des langues ! [Cette page](https://universaldependencies.org/) liste pour chaque langue ses spécificités.)\n",
    "   * Pour la racinisation, l'algorithme de Porter adapté au français est disponible dans le `nltk.stem.SnowBallStemmer`\n",
    "   * Etudiez quelques documents pour vérifier la qualité des étiquettes morphosyntaxiques, et la forme racinisée des mots.\n",
    "   * Les étiquettes morphosyntaxiques vous semblent-elle correcte ? Si non donnez 2 exemples de mauvais étiquetage et une hypothèse.\n",
    "\n",
    "- Votre réponse\n",
    "\n",
    "5. Grâce à ces fonctions pré-traitez tous les documents.\n",
    "6. Quel sujet est commun à chaque quinquennat étudié ? (concatener les discours de chaque quinquennat et regarder les mot communs)\n",
    "\n",
    "- Votre réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d06653e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /workspaces/Linux/.venv/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /workspaces/Linux/.venv/lib/python3.12/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /workspaces/Linux/.venv/lib/python3.12/site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /workspaces/Linux/.venv/lib/python3.12/site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in /workspaces/Linux/.venv/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "693de9b9-2263-4936-8f35-bfb7d9f663d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['achat']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[\"d'achat\"]\n",
      "[]\n",
      "[]\n",
      "[\"d'achat\"]\n",
      "[\"d'achat\"]\n",
      "[]\n",
      "[\"d'achat\", \"d'achat\"]\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Q3\n",
    "import os\n",
    "from glob import glob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Les opérateurs de glob correspondent à l'opérateur * dans les commandes bash.\n",
    "\n",
    "def load_document(doc):\n",
    "    # utilisez ma_str.split, ma_str.join et ma_str.strip pour supprimer l'URL et le titre des documents\n",
    "    lines = doc.split('\\n')\n",
    "    # Supposons que le titre est la première ligne et l'URL la deuxième ligne\n",
    "    content_lines = lines[2:]  # On ignore les deux premières lignes\n",
    "    doc = '\\n'.join(content_lines).strip()\n",
    "    return doc\n",
    "\n",
    "data = []\n",
    "for file_name in glob('discours_voeux/*.txt'): \n",
    "    with open(file_name) as f:\n",
    "        # On l'ouvre et on le lis\n",
    "        doc = f.read()\n",
    "        doc = load_document(doc)\n",
    "        tokens = word_tokenize(doc, language='french')\n",
    "        data.append(tokens)\n",
    "        current_chat = [word for word in tokens if 'chat' in word.lower()]\n",
    "        print(current_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a212a2",
   "metadata": {},
   "source": [
    "Q3 reponse: Dans le question précédant on a token \"achat.\",maintenant on a token \"achat\"ou \"d'achat\",réussi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9a5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Françaises', 'NNS', 'français')\n",
      "(',', ',', ',')\n",
      "('Français', 'NNP', 'franc')\n",
      "(',', ',', ',')\n",
      "('Mes', 'NNP', 'me')\n",
      "('chers', 'NNS', 'cher')\n",
      "('compatriotes', 'VBZ', 'compatriot')\n",
      "('de', 'IN', 'de')\n",
      "(\"l'hexagone\", 'FW', \"l'hexagon\")\n",
      "('et', 'FW', 'et')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndef load_document(doc):\\n    # utilisez ma_str.split, ma_str.join et ma_str.strip pour supprimer l\\'URL et le titre des documents\\n    lines = doc.split(\\'\\n\\')\\n    # Supposons que le titre est la première ligne et l\\'URL la deuxième ligne\\n    content_lines = lines[2:]  # On ignore les deux premières lignes\\n    doc = \\'\\n\\'.join(content_lines).strip()\\n    return doc\\n\\n\\ndata = []\\nfor file_name in glob(\\'discours_voeux/*.txt\\'): \\n    with open(file_name) as f:\\n        # On l\\'ouvre et on le lis\\n        doc = f.read()\\n        doc = load_document(doc)\\n\\n        # Q3\\n        tokens = word_tokenize(doc, language=\\'french\\')\\n\\n        name = os.path.basename(file_name)\\n        name = name.split(\\'.\\')[0]\\n        quin, year, pres = name.split(\\'-\\')\\n        data.append((quin, year, pres, doc))\\n\\n        all_words_flat = [word for (quin, year, pres, tokens) in data for word in tokens]\\n        check_chat = [t for t in all_words_flat if \\'chat\\' in t.lower()]\\n        print(f\"包含 \\'chat\\' 的 Tokens 检查: {check_chat[:20]}\")\\n        print(check_chat)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def pretreat(tokens):\n",
    "    tags = nltk.pos_tag(tokens) \n",
    "    stemmer = SnowballStemmer(\"french\")\n",
    "    result = []\n",
    "    \n",
    "    for word, tag in tags:\n",
    "        stem = stemmer.stem(word)\n",
    "        result.append((word, tag, stem))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def load_document(doc):\n",
    "    lines = doc.split('\\n')\n",
    "    content_lines = lines[2:]\n",
    "    doc = '\\n'.join(content_lines).strip()\n",
    "    return doc\n",
    "\n",
    "data = []\n",
    "files = glob('discours_voeux/*.txt')\n",
    "\n",
    "for file_name in files: \n",
    "    with open(file_name) as f:\n",
    "        doc = f.read()\n",
    "        doc = load_document(doc)\n",
    "        tokens = word_tokenize(doc, language='french')\n",
    "        processed_doc = pretreat(tokens)\n",
    "        data.append(processed_doc)\n",
    "# Afficher les 10 premiers éléments du premier document traité\n",
    "for item in data[0][:10]:\n",
    "    print(item)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def load_document(doc):\n",
    "    # utilisez ma_str.split, ma_str.join et ma_str.strip pour supprimer l'URL et le titre des documents\n",
    "    lines = doc.split('\\n')\n",
    "    # Supposons que le titre est la première ligne et l'URL la deuxième ligne\n",
    "    content_lines = lines[2:]  # On ignore les deux premières lignes\n",
    "    doc = '\\n'.join(content_lines).strip()\n",
    "    return doc\n",
    "\n",
    "\n",
    "data = []\n",
    "for file_name in glob('discours_voeux/*.txt'): \n",
    "    with open(file_name) as f:\n",
    "        # On l'ouvre et on le lis\n",
    "        doc = f.read()\n",
    "        doc = load_document(doc)\n",
    "\n",
    "        # Q3\n",
    "        tokens = word_tokenize(doc, language='french')\n",
    "        \n",
    "        name = os.path.basename(file_name)\n",
    "        name = name.split('.')[0]\n",
    "        quin, year, pres = name.split('-')\n",
    "        data.append((quin, year, pres, doc))\n",
    "\n",
    "        all_words_flat = [word for (quin, year, pres, tokens) in data for word in tokens]\n",
    "        check_chat = [t for t in all_words_flat if 'chat' in t.lower()]\n",
    "        print(f\"包含 'chat' 的 Tokens 检查: {check_chat[:20]}\")\n",
    "        print(check_chat)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d9ee5",
   "metadata": {},
   "source": [
    "Q4 reponse: 1.('compatriotes', 'VBZ', 'compatriot') ici c'est Verbe(VBZ) ,mais \"compatriotes\" doit nom NNS.\n",
    "            2.('et', 'FW', 'et')  ici c'est emprunt(FW)，mais \"et\" doit conjonction\n",
    "            Parce que NLTK utilise le modèle anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76c6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 所有任期的共同话题 (Sujets communs) ---\n",
      "['beaucoup', \"qu'el\", 'caus', 'qualit', 'gouvern', 'confianc', 'plein', 'veux', 'élect', 'décid', 'davantag', 'faut', 'combat', 'pouvon', 'mond', 'port', 'pris', 'seront', 'compétit', 'chanc', 'simplifi', 'mieux', 'européen', 'compatriot', 'certain', 'vivr', 'puis', 'suis', 'présent', 'travaill', 'surtout', 'tant', 'fait', 'jour', 'produit', 'avec', 'part', 'propr', 'renforc', 'innov', 'travail', 'doubl', 'oeuvr', \"l'etat\", 'assum', 'princip', 'rest', 'enfant', 'peut', 'encor', 'cris', 'depuis', 'derni', 'social', 'égal', 'ensembl', 'format', 'nombreux', 'somm', 'arrêt', 'rétabl', 'jeun', 'comm', 'indépend', 'destin', 'sécur', 'solidair', 'nombr', 'énerg', 'international', 'décis', 'arriv', 'proteg', 'faibl', 'justic', 'quant', 'défens', 'servic', 'famill', 'cher', 'unit', 'exprim', 'avoir', 'laquel', 'nation', 'pourquoi', 'donc', 'devon', 'fiscal', 'chang', 'regl', 'difficil', \"c'est\", 'meilleur', 'contr', 'partout', 'votr', 'malgr', 'peur', 'républ', 'droit', 'doit', 'difficult', 'professionnel', 'mesur', 'effort', 'emplois', 'effet', 'mettr', 'soit', 'dout', 'sais', 'soir', 'polit', 'moment', 'transport', 'dépend', 'continent', 'auss', 'voeux', 'system', 'ceux', 'mainten', 'agir', 'object', 'rien', 'sent', 'épreuv', 'ains', 'nouveau', 'sort', 'sauv', 'avant', 'réform', 'vocat', \"l'écol\", 'pouvoir', 'aven', 'détermin', 'divis', 'républicain', 'avez', 'souhait', 'volont', 'résultat', 'vient', 'avanc', \"l'intérêt\", 'fois', 'chacun', 'larg', 'vérit', 'français', 'dont', 'rassembl', 'menac', 'commenc', 'puiss', 'climat', 'sont', 'compt', 'mandat', 'être', 'form', 'respect', 'pens', 'grand', 'montr', 'fond', 'paix', 'partag', 'just', 'reven', 'laïcit', 'particuli', 'fonction', 'societ', 'partenair', 'parl', 'publiqu', 'permettront', \"m'av\", 'engag', 'mobilis', 'enseign', 'continu', 'mois', 'peupl', 'enfin', 'term', 'reconnaiss', 'devoir', 'capac', 'public', 'capabl', 'planet', 'tâch', 'plac', 'industriel', 'chômag', 'permettr', 'prix', \"l'europ\", \"l'un\", 'transform', 'rendr', 'lutt', 'confi', 'vital', 'collect', 'fort', 'amélior', 'nouvel', 'tous', 'model', 'savon', 'début', 'déjà', 'possibl', 'franc', 'forc', 'temp', 'cultur', 'faison', 'fronti', 'femm', 'voilà', 'choix', 'valeur', 'réuss', 'croissanc', 'principal', 'crois', 'construct', 'util', 'esprit', 'langu', 'décembr', 'assur', 'solid', 'solidar', 'allon', 'deux', \"d'un\", 'emploi', 'unis', 'souverain', \"n'est\", 'quand', 'libert', 'cinq', 'alor', 'entrepris', 'concitoyen', 'trop', 'gagn', 'premi', 'nécessair', 'parc', 'fiert', 'entier', 'elle', 'grâc', 'financi', 'bien', 'prochain', 'chaqu', 'autr', 'leur', 'coût', 'création', 'respons', 'accompl', 'résist', 'puissanc', \"l'égal\", 'celui', 'personnel', 'mati', 'aucun', 'fair', \"d'assur\", 'situat', 'toujour', \"d'invest\", 'permet', 'soldat', 'retrait', 'numer', 'massif', \"l'ann\", 'mais', 'apres', 'moyen', 'longtemp', 'économ', 'profond', 'europ', 'surmont', 'seul', 'ministr', 'avon', 'raison', 'plan', 'défendr', \"d'êtr\", 'réduir', 'souten', 'danger']\n"
     ]
    }
   ],
   "source": [
    "# Q5\n",
    "\n",
    "def load_document(doc):\n",
    "    lines = doc.split('\\n')\n",
    "    return '\\n'.join(lines[2:]).strip()\n",
    "\n",
    "def pretreat(tokens):\n",
    "    stemmer = SnowballStemmer(\"french\")\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "files = glob('discours_voeux/*.txt')\n",
    "quin_data = {}\n",
    "for file_name in files: \n",
    "    with open(file_name) as f:\n",
    "        # A. 读取清洗\n",
    "        doc = load_document(f.read())\n",
    "        \n",
    "        # B. 分词 & 提取词根\n",
    "        tokens = word_tokenize(doc, language='french')\n",
    "        stems = pretreat(tokens)\n",
    "        \n",
    "        # C. 获取任期号 (文件名: 1-1995-Chirac.txt -> 拿 '1')\n",
    "        name = os.path.basename(file_name)\n",
    "        quin_id = name.split('-')[0]\n",
    "\n",
    "        name = name.split('.')[0]\n",
    "        quin, year, pres = name.split('-')\n",
    "        data.append((quin, year, pres, doc))\n",
    "\n",
    "        if quin_id not in quin_data:\n",
    "            quin_data[quin_id] = set() # 创建新集合\n",
    "        \n",
    "        # 把词根加入集合 (set会自动去重)\n",
    "        quin_data[quin_id].update(stems)\n",
    "\n",
    "# 3. 找共同点 (Question 6)\n",
    "# 先拿出第一个任期的所有词\n",
    "common_words = list(quin_data.values())[0]\n",
    "\n",
    "# 和后面所有任期的词取交集 (&)\n",
    "for current_set in quin_data.values():\n",
    "    common_words = common_words & current_set\n",
    "\n",
    "# 4. 打印结果 (过滤掉 \"le\", \"de\" 这种无意义的词)\n",
    "ignore = ['le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'est', 'pour', 'que', 'qui', 'dans', 'nous', 'vous', 'notr', 'tout', 'plus', 'anné', 'voeu']\n",
    "\n",
    "# 只显示长度大于3且不在忽略列表里的词\n",
    "final_result = [w for w in common_words if len(w) > 3 and w not in ignore]\n",
    "\n",
    "print(\"--- 所有任期的共同话题 (Sujets communs) ---\")\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03423a-529d-44d9-8b0c-bd4ff8fdeec6",
   "metadata": {},
   "source": [
    "### Sauvegarde sur le disque\n",
    "\n",
    "Le choix du format de stockage de document pré-traité n'est pas trivial, nous proposons ici d'utilise le format jsonl qui permet de sauvegarder les données au format json. Cette n'est ni la meilleure ni la seules, tout dépend de l'utilisation qui sera faite des données, de la taille des fichiers, etc.\n",
    "\n",
    "Assurez vous que vous pouvez charger vos données après les avoir sauvegardé !\n",
    "\n",
    "```python\n",
    "# Sauvegarder les données\n",
    "with open('path/to/file.jsonl', 'w') as f:\n",
    "    for doc in documents:\n",
    "        # Chaque ligne devient un dictionnaire python\n",
    "        r = {\n",
    "            'year': (), 'quinq': (), 'pres': (),\n",
    "            'doc': (), # le document original non prétraité\n",
    "            'doc_pret': () # la version prétraitée du document\n",
    "        }\n",
    "        # Chaque dictionnaire est serialisé en json\n",
    "        f.write(json.dumps(r) + '\\n')\n",
    "\n",
    "# Charger les données\n",
    "with open('path/to/file.jsonl') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed533ed-8cb1-46f8-9357-47bdc452b6c6",
   "metadata": {},
   "source": [
    "## Exercice 2: Exploration des données\n",
    "\n",
    "En utilisant les fichiers précédemment pré-traités, extrayez et visualisez à l'aide de graphiques ou forme textuelle les informations suivantes:\n",
    "\n",
    "1. la longueur des documents en termes de caractères et de mots pour l'ensemble du corpus et par président\n",
    "   * y a-t-il une différence notable entre chaque président ?\n",
    "\n",
    "|        | Carac | Mots  |\n",
    "|--------|-------|-------|\n",
    "|Corpus  |       |       |\n",
    "|Sarkozy |       |       |\n",
    "|Hollande|       |       |\n",
    "|Macron  |       |       |\n",
    "\n",
    "\n",
    "2. la fréquence des mots et formes racinées pour l'ensemble du corpus\n",
    "    - Y a-t-il une différence les 10 premieres racines et mots ? Laquelle ?\n",
    "- Votre réponse\n",
    "\n",
    "3. Faire un graphique représentant la fréquence des mots par ordre décroissant (avec une échelle logarithmique).\n",
    "   * Vous devez observer la [loi de Zipf](https://fr.wikipedia.org/wiki/Loi_de_Zipf#Gen%C3%A8se): seuls quelques mots constituent une grande partie du corpus.\n",
    "   * Ces mots qui apparaissent souvent n'apportent généralement que peu d'information, on dit que ce sont des mots vides (stopwords), contrairement aux mots plein (en général noms, adjectifs, verbes, ...). Il est courant de les filtrer pour ne pas surcharger les modèles. Des listes de stopwords sont disponibles dans `nltk.corpus.stopwords.words`, chaque bibliothèque de TAL possède en général sa liste.\n",
    "   * /!\\\\ Il est important d'utiliser une liste compatible avec le tokeniseur utilisé. En effet, il est fréquent que le tokeniseur segmente `\"puisqu'elle\"` en `[\"puisqu'\", 'elle']` mais que la liste de mots vide contienne `puisqu'elle` mais pas `puisqu'` !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbad1e-15e4-45e3-9f37-510efe5b88d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Name | Carac (Moy) | Mots (Moy)\n",
      "----------------------------------------\n",
      "Corpus | 9988 | 1870\n",
      "Macron | 13443 | 2538\n",
      "Hollande | 8091 | 1515\n",
      "Sarkozy | 7049 | 1291\n",
      "========================================\n",
      "正在统计...\n",
      "单词 (无标点)        | 词根             \n",
      "-----------------------------------\n",
      "de              | de             \n",
      "la              | le             \n",
      "et              | la             \n",
      "à               | et             \n",
      "nous            | à              \n",
      "les             | nous           \n",
      "pour            | pour           \n",
      "le              | en             \n",
      "en              | qui            \n",
      "qui             | que            \n",
      "文件: q_2017-2019-Macron.txt | 包含 'chat' 的 Tokens: []\n",
      "文件: q_2022-2024-Macron.txt | 包含 'chat' 的 Tokens: []\n",
      "文件: q_2017-2017-Macron.txt | 包含 'chat' 的 Tokens: []\n",
      "文件: q_2017-2020-Macron.txt | 包含 'chat' 的 Tokens: []\n",
      "文件: q_2017-2021-Macron.txt | 包含 'chat' 的 Tokens: ['achat']\n",
      "文件: q_2022-2023-Macron.txt | 包含 'chat' 的 Tokens: []\n",
      "文件: q_2012-2016-Hollande.txt | 包含 'chat' 的 Tokens: []\n",
      "文件: q_2012-2015-Hollande.txt | 包含 'chat' 的 Tokens: []\n",
      "文件: q_2012-2012-Hollande.txt | 包含 'chat' 的 Tokens: []\n",
      "文件: q_2012-2013-Hollande.txt | 包含 'chat' 的 Tokens: [\"d'achat\"]\n",
      "文件: q_2007-2008-Sarkozy.txt | 包含 'chat' 的 Tokens: []\n",
      "文件: q_2017-2018-Macron.txt | 包含 'chat' 的 Tokens: []\n",
      "文件: q_2007-2007-Sarkozy.txt | 包含 'chat' 的 Tokens: [\"d'achat\"]\n",
      "文件: q_2007-2010-Sarkozy.txt | 包含 'chat' 的 Tokens: [\"d'achat\"]\n",
      "文件: q_2012-2014-Hollande.txt | 包含 'chat' 的 Tokens: []\n",
      "文件: q_2007-2011-Sarkozy.txt | 包含 'chat' 的 Tokens: [\"d'achat\", \"d'achat\"]\n",
      "文件: q_2007-2009-Sarkozy.txt | 包含 'chat' 的 Tokens: []\n"
     ]
    }
   ],
   "source": [
    "# Votre réponse\n",
    "#Q1\n",
    "president_data = {}\n",
    "corpus_chars=[]\n",
    "corpus_mots=[]\n",
    "files = glob('discours_voeux/*.txt')\n",
    "for file_name in files: \n",
    "    with open(file_name) as f:\n",
    "        # On l'ouvre et on le lis\n",
    "        doc = f.read()\n",
    "        doc = load_document(doc)\n",
    "        nb_chars = len(doc)\n",
    "        corpus_chars.append(nb_chars)\n",
    "        tokens = word_tokenize(doc, language='french')\n",
    "        nb_words = len(tokens)\n",
    "        corpus_mots.append(nb_words)\n",
    "        name = os.path.basename(file_name)\n",
    "        name = name.split('-')[2].replace('.txt', '')\n",
    "        \n",
    "        # préparer la structure de données pour chaque président\n",
    "        if name not in president_data:\n",
    "            president_data[name] = {'chars': [], 'words': []}\n",
    "        \n",
    "        president_data[name]['chars'].append(nb_chars)\n",
    "        president_data[name]['words'].append(nb_words)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"{'Name'} | {'Carac (Moy)'} | {'Mots (Moy)'}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if len(corpus_chars) > 0:\n",
    "    avg_corpus_chars = sum(corpus_chars) / len(corpus_chars)\n",
    "    avg_corpus_words = sum(corpus_mots) / len(corpus_mots)\n",
    "    print(f\"{'Corpus'} | {int(avg_corpus_chars)} | {int(avg_corpus_words)}\")\n",
    "\n",
    "\n",
    "names = list(president_data.keys())\n",
    "for name in names:\n",
    "    chars_list = president_data[name]['chars']\n",
    "    words_list = president_data[name]['words']\n",
    "\n",
    "    avg_chars = sum(chars_list) / len(chars_list)\n",
    "    avg_words = sum(words_list) / len(words_list)\n",
    "    \n",
    "    print(f\"{name} | {int(avg_chars)} | {int(avg_words)}\")\n",
    "\n",
    "print(\"=\"*40)\n",
    "\n",
    "#Q2\n",
    "from collections import Counter\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"french\")\n",
    "files = glob('discours_voeux/*.txt')\n",
    "freq_mots = Counter()\n",
    "freq_racines = Counter()\n",
    "\n",
    "for file_name in files:\n",
    "    with open(file_name) as f:\n",
    "        # 读文件 -> 切行 -> 丢掉前两行 -> 拼回字符串\n",
    "        text = '\\n'.join(f.read().split('\\n')[2:])\n",
    "        \n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    \n",
    "    clean_words = [t.lower() for t in tokens if t.isalnum()]\n",
    "    \n",
    "    # --- 统计 ---\n",
    "    freq_mots.update(clean_words)\n",
    "    # 顺便把词根也算出来统计了\n",
    "    freq_racines.update([stemmer.stem(w) for w in clean_words])\n",
    "\n",
    "# --- 打印结果 ---\n",
    "print(f\"{'单词 (无标点)':<15} | {'词根':<15}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# zip 可以把两个列表“拉链”在一起同时遍历\n",
    "for (m, c1), (r, c2) in zip(freq_mots.most_common(10), freq_racines.most_common(10)):\n",
    "    print(f\"{m:<15} | {r:<15}\")\n",
    "\n",
    "\n",
    "#Q3\n",
    "files = glob('discours_voeux/*.txt')\n",
    "for file_name in files: \n",
    "    with open(file_name) as f:\n",
    "        doc = f.read()\n",
    "        doc = load_document(doc)\n",
    "        tokens = word_tokenize(doc, language='french')\n",
    "        current_chat = [word for word in tokens if 'chat' in word.lower()]\n",
    "        name = os.path.basename(file_name)\n",
    "        print(f\"文件: {name} | 包含 'chat' 的 Tokens: {current_chat}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7e823-08b8-47e7-8bca-8882114c20d8",
   "metadata": {},
   "source": [
    "4. Combien de mots n'apparraissent qu'une seule fois ? On appelle ces mots des hapax (hapax legomena)\n",
    "- Votre réponse\n",
    "  \n",
    "3. Afficher les 10 n-grammes (de 1 à 3) les plus fréquent (la bibliothèque `nltk` permet cela) pour l'ensemble du corpus et par président."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda634b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ac813a-3e76-4e45-8274-5cebf8984284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc7b50-10c3-4a28-af15-ab320379a6db",
   "metadata": {},
   "source": [
    "4. Afficher les 10 noms, verbes, adverbes et adjectifs les plus fréquents pour l'ensemble du corpus et par président"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8bdef92-021b-434d-b808-e03d0d9f87d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b0ef5-0707-43f7-8d93-2449a980e717",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding\n",
    "\n",
    "A l'aide de la bibliothèque [`tokenizers`](https://huggingface.co/docs/tokenizers/index) et du code ci-dessous.\n",
    "\n",
    "1. Comparez la tokenisation en sous-mots du discours de Hollande en 2015 avec les modèles `'camembert/camembert-base'` et `'bert-base-uncased'`.\n",
    "    - Le modèle camembert à été entraîné sur des données en française et bert-base sur des données anglaises.\n",
    "    - Quelle différence observez-vous et formulez une hypothèse.\n",
    "  \n",
    "- Votre réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66768b16-daae-4963-9d25-2d687f243f65",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tokenizers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtokenizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[32m      2\u001b[39m tokenizer = Tokenizer.from_pretrained(MODELE)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tokenizers'"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_pretrained(MODELE)\n",
    "\n",
    "tokenizer_fr = Tokenizer.from_pretrained(\"camembert/camembert-base\")\n",
    "tokens_fr = tokenizer_fr.encode(text).tokens\n",
    "\n",
    "tokenizer_en = Tokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens_en = tokenizer_en.encode(text).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54240f79-f1cd-46c2-b86f-8df8e6a6aae3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtokenizer\u001b[49m.encode(MONTEXTE).tokens\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.encode(MONTEXTE).tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f07637-75cf-47a3-946e-1bc2e428a02f",
   "metadata": {},
   "source": [
    "## Spacy\n",
    "\n",
    "Une autre bibliothèque pour l'analyse de texte est [`spacy`](https://spacy.io/). Sa philosophie est différente de nltk (qui ne travaille qu'avec des listes), avec `spacy` tout est un objet.\n",
    "\n",
    "1. Installez le modèle français pour spacy\n",
    "2. Créez une fonction `pretreat_spacy` qui retourne la même chose, mais n'utilise que spacy. Est-ce que toutes les informations sont disponibles ?\n",
    "3. Y a-t-il des différence dans l'étiquetage morphosyntaxique entre spacy et nltk ?\n",
    "    - Donnez 3 exemples s'il y en a..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3031e0da-a588-4e59-9c47-45d60a08612e",
   "metadata": {},
   "source": [
    "# Analyse textuelle\n",
    "\n",
    "Avec les outils utilisés jusqu'a présent essayez de répondre aux questions suivantes:\n",
    "\n",
    "5. Comment identifier les thèmes principaux abordés par chaque président ?\n",
    "6. Y a-t-il des différence importante de vocabulaire entre Sarkozy et Macron ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6595a3b8-d9e7-422f-b4eb-efdb3c939efd",
   "metadata": {},
   "source": [
    "### Paquets/commandes utiles:\n",
    "\n",
    "* Ensembles en python: `vocab = set('a b b b c'.split()))`, ainsi que les intersection `set('abc') & set('bc')`, difference `set('abc') - set('bc')`, combinaison `set('abc') | set('bc')`.\n",
    "* `collections.Counter`: un dictionnaire qui compte les occurence d'un élement\n",
    "* Mesurer le temps d'execution d'une commande dans un jupyter notebook\n",
    "\n",
    "```\n",
    "%%time  # pour une cellule entière\n",
    "code python\n",
    "\n",
    "%time code python # pour une ligne\n",
    "```\n",
    "\n",
    "* Pour faire des graphiques\n",
    "  * [matplotlib](https://matplotlib.org)\n",
    "  * [pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) offre des moyens assez simple de faire des graphiques.\n",
    "  * [seaborn](https://seaborn.pydata.org) fait de beaux graphiques.\n",
    "\n",
    "```\n",
    "# exemple minimal du graphique de la fonction x^2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(-10, 10), [i**2 for i in range(-10, 10)])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- Pour simplifier les traitements utilisez des compréhensions de liste facile à lire.\n",
    "```python\n",
    "tmp_list = []\n",
    "for t in tokens:\n",
    "    t = t.replace('ü', 'u)\n",
    "    tmp_list.append(t)\n",
    "tokens = tmp_list\n",
    "\n",
    "tokens = [t.replace('ü', 'u') for t in tokens]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef775b-3d29-4fb7-ad4c-416f31849abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
